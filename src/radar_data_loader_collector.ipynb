{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "from typing import List, Optional, Union\n",
    "import os \n",
    "import h5py\n",
    "\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "from agent import Agent\n",
    "from dataset import EpisodesDataset\n",
    "from envs import SingleProcessEnv, MultiProcessEnv\n",
    "from episode import Episode\n",
    "from utils import EpisodeDirManager, RandomHeuristic\n",
    "from datetime import datetime, timedelta \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Radar_Collector: \n",
    "    def __init__(self, env: Union[SingleProcessEnv, MultiProcessEnv], dataset: EpisodesDataset, episode_dir_manager: EpisodeDirManager) -> None:\n",
    "        self.env = env\n",
    "        self.dataset = dataset\n",
    "        self.episode_dir_manager = episode_dir_manager\n",
    "        self.obs = self.env.reset()\n",
    "        self.episode_ids = [None] * self.env.num_envs\n",
    "        self.heuristic = RandomHeuristic(self.env.num_actions)\n",
    "\n",
    "\n",
    "    ##########################################################\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def collect(self, agent: Agent, epoch: int, epsilon: float, should_sample: bool, temperature: float, burn_in: int, *, num_steps: Optional[int] = None, num_episodes: Optional[int] = None, current_date: datetime, radar_dataset_frequency: str):\n",
    "        self.current_date=current_date \n",
    "        self.radar_dataset_frequency=radar_dataset_frequency\n",
    "        self.num_steps=num_steps \n",
    "        self.num_episodes=num_episodes\n",
    "        assert self.env.num_actions == agent.world_model.act_vocab_size\n",
    "        assert 0 <= epsilon <= 1\n",
    "        print(num_steps)\n",
    "        print(num_episodes)\n",
    "        assert (num_steps is None) != (num_episodes is None)\n",
    "        should_stop = lambda steps, episodes: steps >= num_steps if num_steps is not None else episodes >= num_episodes\n",
    "\n",
    "\n",
    "        to_log = []\n",
    "        steps, episodes = 0, 0\n",
    "        returns = []\n",
    "        observations, actions, rewards, dones = [], [], [], []\n",
    "\n",
    "        burnin_obs_rec, mask_padding = None, None\n",
    "\n",
    "        #################################################################################################\n",
    "        ##  This part is about creating episde IDs    \n",
    "        \n",
    "        \n",
    "        if set(self.episode_ids) != {None} and burn_in > 0:\n",
    "            current_episodes = [self.dataset.get_episode(episode_id) for episode_id in self.episode_ids]\n",
    "            segmented_episodes = [episode.segment(start=len(episode) - burn_in, stop=len(episode), should_pad=True) for episode in current_episodes]\n",
    "            mask_padding = torch.stack([episode.mask_padding for episode in segmented_episodes], dim=0).to(agent.device)\n",
    "            burnin_obs = torch.stack([episode.observations for episode in segmented_episodes], dim=0).float().div(255).to(agent.device)\n",
    "            burnin_obs_rec = torch.clamp(agent.tokenizer.encode_decode(burnin_obs, should_preprocess=True, should_postprocess=True), 0, 1)\n",
    "\n",
    "        agent.actor_critic.reset(n=self.env.num_envs, burnin_observations=burnin_obs_rec, mask_padding=mask_padding)\n",
    "        pbar = tqdm(total=num_steps if num_steps is not None else num_episodes, desc=f'Experience collection ({self.dataset.name})', file=sys.stdout)\n",
    "        ##################################################################################################\n",
    "       \n",
    "\n",
    "        #### These two lines should be taken from the CONFIGURATION FILES \n",
    "        #### the trainning data is between the 2008 and 2014, for validation data between 2015 and 2017, and for testing yeras 2018-2020\n",
    "        #### For testing take it now from the user \n",
    "\n",
    "        # start_date = datetime(2009, 5, 1) \n",
    "        # radar_dataset_frequency = \"Monthly\"\n",
    "        # radar_dataset_path\n",
    "        next_date=current_date\n",
    "        while not should_stop(steps, episodes):\n",
    "            print(\"steps:\", steps)\n",
    "            print(\"episodes:\", episodes)\n",
    "            print(current_date)\n",
    "            print(radar_dataset_frequency)\n",
    "            output, next_date = self.Radar_Data_Loader(next_date, radar_dataset_frequency)\n",
    "            print(next_date)\n",
    "            daily_output=output\n",
    "            daily_output = np.array(daily_output)\n",
    "            observations = torch.tensor(daily_output)\n",
    "            observations=observations.unsqueeze(3)\n",
    "            observations= rearrange(torch.FloatTensor(observations).div(255), 'n h w c -> n c h w').to(agent.device)\n",
    "           \n",
    "            \n",
    "            for i in range(len(output)):\n",
    "                observation=observations[i]\n",
    "                observation=observation.unsqueeze(0)\n",
    "                #print(observation.size())\n",
    "                # in the action production I have changed the number of input, output channel and the resolution in the config file of tokenizer \n",
    "                # In the actor critic I have changed two lines \n",
    "                # 1) self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=1) by self.conv1 = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n",
    "                # 2)  assert inputs.ndim == 4 and inputs.shape[1:] == (3, 64, 64) by assert inputs.ndim == 4 and inputs.shape[1:] == (1, 256, 256) \n",
    "               \n",
    "                act = agent.act(observation, should_sample=should_sample, temperature=temperature).cpu().numpy()\n",
    "\n",
    "\n",
    "                _ , reward, done, _ = train_env.step(act)\n",
    "                actions.append(act)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "\n",
    "\n",
    "                new_steps = len(self.env.mask_new_dones)\n",
    "                print(\"new_steps:\", new_steps)\n",
    "                pbar.update(new_steps if num_steps is not None else 0)\n",
    "            if self.env.should_reset():\n",
    "                self.add_experience_to_dataset(observations, actions, rewards, dones)\n",
    "\n",
    "                new_episodes = self.env.num_envs\n",
    "                print(\"new_episode:\", new_episodes)\n",
    "                episodes += new_episodes\n",
    "                pbar.update(new_episodes if num_episodes is not None else 0)\n",
    "\n",
    "                for episode_id in self.episode_ids:\n",
    "                    episode = self.dataset.get_episode(episode_id)\n",
    "                    self.episode_dir_manager.save(episode, episode_id, epoch)\n",
    "                    metrics_episode = {k: v for k, v in episode.compute_metrics().__dict__.items()}\n",
    "                    metrics_episode['episode_num'] = episode_id\n",
    "                    metrics_episode['action_histogram'] = wandb.Histogram(np_histogram=np.histogram(episode.actions.numpy(), bins=np.arange(0, self.env.num_actions + 1) - 0.5, density=True))\n",
    "                    to_log.append({f'{self.dataset.name}/{k}': v for k, v in metrics_episode.items()})\n",
    "                    returns.append(metrics_episode['episode_return'])\n",
    "                self.obs = self.env.reset()\n",
    "                self.episode_ids = [None] * self.env.num_envs\n",
    "                agent.actor_critic.reset(n=self.env.num_envs)\n",
    "                observations, actions, rewards, dones = [], [], [], []\n",
    "                # current_date=next_date\n",
    "                # print(current_date)\n",
    "\n",
    "            ## Add incomplete episodes to dataset, and complete them later.\n",
    "            ## This is not used in our case \n",
    "            # if len(observations) > 0:\n",
    "            #     self.add_experience_to_dataset(observations, actions, rewards, dones)\n",
    "\n",
    "            agent.actor_critic.clear()\n",
    "\n",
    "            metrics_collect = {\n",
    "                '#episodes': len(self.dataset),\n",
    "                '#steps': sum(map(len, self.dataset.episodes)),\n",
    "            }\n",
    "            if len(returns) > 0:\n",
    "                metrics_collect['return'] = np.mean(returns)\n",
    "            metrics_collect = {f'{self.dataset.name}/{k}': v for k, v in metrics_collect.items()}\n",
    "            to_log.append(metrics_collect)\n",
    "            current_date=next_date\n",
    "            return to_log, next_date\n",
    "\n",
    "            # observations.append(self.obs)\n",
    "            # obs = rearrange(torch.FloatTensor(self.obs).div(255), 'n h w c -> n c h w').to(agent.device)\n",
    "            # act = agent.act(obs, should_sample=should_sample, temperature=temperature).cpu().numpy()\n",
    "\n",
    "            # if random.random() < epsilon:\n",
    "            #     act = self.heuristic.act(obs).cpu().numpy()\n",
    "\n",
    "            # self.obs, reward, done, _ = self.env.step(act)\n",
    "\n",
    "            # actions.append(act)\n",
    "            # rewards.append(reward)\n",
    "            # dones.append(done)\n",
    "\n",
    "            # new_steps = len(self.env.mask_new_dones)\n",
    "            # steps += new_steps\n",
    "            # pbar.update(new_steps if num_steps is not None else 0)\n",
    "\n",
    "    ################################################################################\n",
    "    #### This is the function that calls the radar dataset, this function takes the intial date with the radar dataset frequency as inputs\n",
    "    #### and returns back the observations and the next date to start from it the next time as it depends how many steps the env will take everytime \n",
    "\n",
    "    def Radar_Data_Loader(self, start_date, radar_dataset_frequency):\n",
    "        radar_dataset_path=Path('/home/hbi/RAD_NL25_RAP_5min') #### This one should be in config file \n",
    "        year, month, day = start_date.year, start_date.month, start_date.day\n",
    "        output = []\n",
    "        next_date = start_date\n",
    "        \n",
    "        if radar_dataset_frequency == \"Monthly\":\n",
    "            # Determine the next month\n",
    "            next_date = datetime(year, month + 1, 1) if month < 12 else datetime(year + 1, 1, 1)\n",
    "            \n",
    "            # Iterate over days in the current month\n",
    "            while datetime(year, month, day) < next_date:\n",
    "                # Iterate over hours from 00:00 to 23:55\n",
    "                for hour in range(0, 24):\n",
    "                    # Iterate over minutes from 00:00 to 23:55 in 5-minute intervals\n",
    "                    for minute in range(0, 60, 5):\n",
    "                        # Construct the file path\n",
    "                        file_name = f'RAD_NL25_RAP_5min_{year}{month:02d}{day:02d}{hour:02d}{minute:02d}.h5'\n",
    "                        root_direction = os.path.join(radar_dataset_path, str(year), f'{month:02d}', file_name)\n",
    "\n",
    "                        # Load and process the image data\n",
    "                        image = np.array(h5py.File(root_direction)['image1']['image_data'])\n",
    "                        image = image[264:520, 242:498]\n",
    "                        image[image == 65535] = 0\n",
    "                        image = image.astype('float32')\n",
    "                        image = image / 100 * 12\n",
    "                        image = np.clip(image, 0, 128)\n",
    "                        image = image / 40\n",
    "                        output.append(image)\n",
    "                \n",
    "                start_date += timedelta(days=1)\n",
    "                year, month, day = start_date.year, start_date.month, start_date.day\n",
    "                \n",
    "        elif radar_dataset_frequency == \"Weekly\":\n",
    "            # Determine the next week\n",
    "            next_date = start_date + timedelta(weeks=1)\n",
    "            \n",
    "            # Iterate over days in the current week\n",
    "            while start_date < next_date:\n",
    "                # Iterate over hours from 00:00 to 23:55\n",
    "                for hour in range(0, 24):\n",
    "                    # Iterate over minutes from 00:00 to 23:55 in 5-minute intervals\n",
    "                    for minute in range(0, 60, 5):\n",
    "                        # Construct the file path\n",
    "                        file_name = f'RAD_NL25_RAP_5min_{year}{month:02d}{day:02d}{hour:02d}{minute:02d}.h5'\n",
    "                        root_direction = os.path.join(radar_dataset_path, str(year), f'{month:02d}', file_name)\n",
    "\n",
    "                        # Load and process the image data\n",
    "                        image = np.array(h5py.File(root_direction)['image1']['image_data'])\n",
    "                        image = image[264:520, 242:498]\n",
    "                        image[image == 65535] = 0\n",
    "                        image = image.astype('float32')\n",
    "                        image = image / 100 * 12\n",
    "                        image = np.clip(image, 0, 128)\n",
    "                        image = image / 40\n",
    "                        output.append(image)\n",
    "                \n",
    "                start_date += timedelta(days=1)\n",
    "                year, month, day = start_date.year, start_date.month, start_date.day\n",
    "        \n",
    "        elif radar_dataset_frequency == \"Daily\":\n",
    "            # Iterate over hours from 00:00 to 23:55\n",
    "            for hour in range(0, 24):\n",
    "                # Iterate over minutes from 00:00 to 23:55 in 5-minute intervals\n",
    "                for minute in range(0, 60, 5):\n",
    "                    # Construct the file path\n",
    "                    file_name = f'RAD_NL25_RAP_5min_{year}{month:02d}{day:02d}{hour:02d}{minute:02d}.h5'\n",
    "                    root_direction = os.path.join(radar_dataset_path, str(year), f'{month:02d}', file_name)\n",
    "\n",
    "                    # Load and process the image data\n",
    "                    image = np.array(h5py.File(root_direction)['image1']['image_data'])\n",
    "                    image = image[264:520, 242:498]\n",
    "                    image[image == 65535] = 0\n",
    "                    image = image.astype('float32')\n",
    "                    image = image / 100 * 12\n",
    "                    image = np.clip(image, 0, 128)\n",
    "                    image = image / 40\n",
    "                    output.append(image)\n",
    "            \n",
    "            next_date = start_date + timedelta(days=1)\n",
    "        \n",
    "        return output, next_date\n",
    "        #######################################################################################################################################################\n",
    "\n",
    "        #######################################################################################################################################################\n",
    "        #### This function is changed to match the radar dataset as in iris they collect each observation, action reward and done and saved them as one episode \n",
    "        #### However, in our radar dataset we want to save them in a Daily, Weeekly and Monthly, thus each Day,Week or Month is an Epiosde. This means that we \n",
    "        #### don't have to iterate over the number of observations, rewards and dones to save the episode, but just convert the data to Tensors and save them \n",
    "        #### \n",
    "\n",
    "    def add_experience_to_dataset(self, observations: List[np.ndarray], actions: List[np.ndarray], rewards: List[np.ndarray], dones: List[np.ndarray]) -> None:\n",
    "        observations=observations\n",
    "        observations=observations.permute(1, 0, 2, 3) # Basically the channel should be first \n",
    "        actions = torch.LongTensor(actions)\n",
    "        actions=actions.permute(1,0)\n",
    "        rewards=torch.tensor(rewards)\n",
    "        rewards=rewards.permute(1,0)\n",
    "        ends=torch.LongTensor(dones)\n",
    "        ends=ends.permute(1,0)\n",
    "        dones=torch.LongTensor(dones)\n",
    "        dones=dones.permute(1,0)\n",
    "        dones_array = np.array(dones)  # Convert the list to a NumPy array\n",
    "        mask_padding=torch.ones(dones_array.shape[1], dtype=torch.bool)\n",
    "        mask_padding=mask_padding.unsqueeze(1)\n",
    "        mask_padding=mask_padding.permute(1,0)\n",
    "        assert len(observations) == len(actions) == len(rewards) == len(dones)\n",
    "        episode = Episode(\n",
    "            observations,  # channel-first\n",
    "            actions,\n",
    "            rewards,\n",
    "            ends=dones,\n",
    "            mask_padding=torch.ones(dones_array.shape[0], dtype=torch.bool),\n",
    "        )\n",
    "        ## This should be CHANGED if we want to have another episode ids but it creates and works\n",
    "        if self.episode_ids[0] is None:\n",
    "            self.episode_ids[0] = self.dataset.add_episode(episode)\n",
    "        else:\n",
    "            self.dataset.update_episode(self.episode_ids[0], episode)\n",
    "    ###################################################################################################################################################    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
