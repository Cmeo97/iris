_target_: models.tokenizer.OCTokenizer

vocab_size: 512
embed_dim: 64
encoder:
  _target_: models.tokenizer.SAEncoder
  config:
    _target_: models.tokenizer.OCEncoderDecoderConfig
    resolution: 64
    in_channels: 3
    z_channels: 32
    ch: 64
    ch_mult: [1, 1, 1, 1, 1]
    num_res_blocks: 2
    attn_resolutions: [8, 16]
    out_ch: 4
    dropout: 0.0
    dec_input_dim: 64 # need to match embed_dim if no pre/post_process_conv
    dec_hidden_dim: 32
decoder:
  _target_: models.tokenizer.SpatialBroadcastDecoder
  config: ${..encoder.config}
slot_attn:
  _target_: models.tokenizer.SlotAttention
  config:
    _target_: models.tokenizer.SAConfig
    num_slots: 4 # num_tokens = num_slots * tokens_per_slot
    tokens_per_slot: 1
    iters: 3
    channels_enc: 32
    token_dim: 64 # slot_dim so